{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Database playground","text":"<p>info</p> <ul> <li>Updated 8/20/2022 - postgres content</li> <li>Updated 06/2025 - Puzzles and Postgresql on k8s</li> <li>Update 09/2025 - reorganize content, add Debezium lab</li> </ul> <p>This repository supports simple studies for database playground (SQL, JPA) using technologies like db2, postgresql. duckdb. It is completed by study of some change data capture to move data from database to kafka topic.</p>"},{"location":"#pure-sql-play","title":"Pure SQL play","text":"<p>Sql Basics, followed by intermediate queries and more complex challenges.</p>"},{"location":"#technologies","title":"Technologies","text":"<ul> <li>Deeper dive into Postgresql.</li> <li>Debezium for CDC</li> </ul>"},{"location":"couchdb/","title":"Apache Couchdb","text":""},{"location":"couchdb/#getting-started","title":"Getting started","text":"<p>Docker image</p> <ul> <li>Start with</li> </ul> <pre><code>docker run -p 5984:5984 --rm -v $(pwd)/database:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=myc0uch --name couchdb couchdb\n</code></pre> <ul> <li>Access to the user interface: Futon</li> <li>Verify the installation: http://localhost:5984/_utils/#/verifyinstall</li> <li>See a simple tour guide</li> </ul>"},{"location":"couchdb/#some-concepts","title":"Some concepts","text":"<ul> <li>To query documents, CouchDB uses predefined map and reduce functions, called view. Map functions are called once with each document as the argument. The function can choose to skip the document altogether or emit one or more view rows as key/value pairs.</li> <li>When writing CouchDB map functions, your primary goal is to build an index that stores related data under nearby keys</li> </ul>"},{"location":"couchdb/#ibm-cloudant-on-cloud","title":"IBM Cloudant On Cloud","text":"<p>Create a cloudant service and use the Dashboard to create database. Then get credentials to be integrated into the client code.</p>"},{"location":"couchdb/#tools","title":"Tools","text":"<p>Code to create records into couchdb from json file in python.</p> <p>https://702e23bc.us-south.apigw.appdomain.cloud/guestbook</p>"},{"location":"db2/","title":"DB2","text":""},{"location":"db2/#docker","title":"Docker","text":"<p>See instructions here</p> <pre><code># start\ndocker run -itd --name mydb2 --privileged=true -p 50000:50000 -e LICENSE=accept -e DB2INST1_PASSWORD=&lt;choose an instance password&gt; -e DBNAME=testdb -v $(pwd)/db2data:/database ibmcom/db2\n\n# Log on the container\ndocker exec -ti db2 bash -c \"su - db2inst1\"\n\n# Start db2 CLI\ndb2\n\n(c) Copyright IBM Corporation 1993,2007\nCommand Line Processor for DB2 Client 11.5.0.0\n\nYou can issue database manager commands and SQL statements from the command \nprompt.\n\ndb2 connect reset \n</code></pre>"},{"location":"db2/#useful-commands","title":"Useful commands","text":"<pre><code># Create the sample DB; creating tables and data in schema \"DB2INST1\".\ndb2 sample -force -sql\n# list current DB\ndb2 list database directory \n# Connect to it\ndb2 connect to sample user db2inst1\n# list tables\n db2 list tables\n# Look at the structure of a table\ndb2 describe table DB2INST1.ORDEREVENTS\n</code></pre>"},{"location":"db2/#db2-on-cloud","title":"DB2 on cloud","text":"<p>Once the service is created get the credential, user, jdbc url and password. Then to access it from a pod on OpenShift, define a secret with those variable encoded with base64</p>"},{"location":"db2/#db2-in-openshift-with-operator","title":"DB2 in OpenShift with operator","text":"<ul> <li>Install the DB2 operator from IBM Catalog: Operator Catalog product documentation</li> <li>create a namespace: eda-dba</li> <li>install the operator from the operator hub</li> <li>Be sure to add <code>ibm-registry</code> secret with the ibm entitled key for the given user.</li> </ul> <pre><code>oc create secret docker-registry ibm-registry   \\\n    --docker-server=cp.icr.io                   \\\n    --docker-username=cp                        \\\n    --docker-password=${ENTITLEDKEY}            \\\n    --docker-email=${USER}                      \\\n    --namespace=${NAMESPACE}\n</code></pre> <ul> <li>Create an instance using the good storage class and modifying the account as:</li> </ul> <pre><code>  license:\n    accept: true\n  account:\n    privileged: true\n    imagePullSecrets:\n      - ibm-registry    \n  version: \"11.5.5.0\"\n  size: 1\n</code></pre> <p>Once the instance is created there are 4 pods running for each instance and one for the operator.</p> <pre><code>c-db2ucluster-sample-db2u-0                   1/1       Running     0          34m\nc-db2ucluster-sample-etcd-0                   1/1       Running     0          35m\nc-db2ucluster-sample-ldap-54b75695d5-dr8xr    1/1       Running     0          37m\nc-db2ucluster-sample-tools-79c8f7659c-znpkw   1/1       Running     0          37m\n</code></pre> <ul> <li>When remote connected to the db2u pod, we can see the mounted volume from the PVC Instruction for lab to prepare a VM for OpenShift and DB2 but may be relevant.</li> </ul> <p>IBM DB@ github</p>"},{"location":"mongodb/","title":"MongoDB","text":"<p>Open source document oriented database designed to scale. It\u00a0stores parsed json doc or bson. For production deployments mongodb uses distributed servers with replication and automatic master failover</p> <p>It supports:</p> <ul> <li>complex queries express as json document</li> <li>evantual consistency</li> <li>malleable schema: can easily change at runtime</li> <li>both large and small scale</li> <li>Automatic sharding (partition data sets over multiple machines) distributes collection data across machines.</li> </ul> <p>Some key concepts - A database holds a set of collections.\u00a0 - A collection holds a set of documents.\u00a0 - A document is a set of key-value pairs.\u00a0 - Documents have dynamic schema. - mongo is the interactive shell to access the DB - application uses driver to access the DB</p> <p>Database for MongoDB Product documentation on IBM cloud See this quarkus tutorial for orm mongo with panache</p>"},{"location":"mongodb/#docker-compose","title":"Docker compose","text":"<pre><code>mongo:\n    container_name: mongo\n    image: mongo\n    restart: always\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: root\n      MONGO_INITDB_ROOT_PASSWORD: example\n    volumes:\n      - ./data:/data/db\n    ports:\n      - 27017:27017\n</code></pre>"},{"location":"mongodb/#mongo-cli","title":"Mongo CLI","text":"<pre><code>mongo --username root --password example\nMongoDB shell version v4.4.5\n\n&gt; show dbs\n local\u00a0\u00a0 0.078125GB\n &gt; use mydb\n switched to db mydb\n &gt; db\n mydb\n &gt; db.createCollection('items')\n &gt;  show collections\nitems\n</code></pre> <p>A collection is a group of related documents that have a set of shared common indexes. Collections are analogous to a table in relational databases. You do not need to create a collection before inserting data.\u00a0When you insert the first document using the command like db.testdata.insert(ajson) \u00a0the mongod will create both the database instance and the collection</p> <p>All MongoDB documents must have an <code>_id</code> field with has a unique value and is considered as primary key. The <code>find</code> method is used to get the content of a collection. The results are grouped by 20, and the command <code>it</code> gives the next page for the result set.\u00a0</p>"},{"location":"mongodb/#connect-to-database-for-mongo","title":"Connect to database for mongo","text":"<ul> <li>Connecting with mongo shell</li> <li> <p>mongo shell interface to MongoDB. Downloadable from  MongoDB Database Tools. 07/01/2020 put them in my <code>~/bin</code></p> </li> <li> <p>Get the TLS certificate</p> </li> </ul> <pre><code>ibmcloud cdb deployment-cacert gse-eda-mongo\n</code></pre>"},{"location":"mysql/","title":"MySQL","text":""},{"location":"mysql/#order-and-customer-database-mysql","title":"Order and Customer database MySQL","text":"<ul> <li>Run mysql in docker:</li> </ul> <pre><code>docker run -it --rm --name mysql -v ../datadir/mysql:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=jb -e MYSQL_USER=admin -e MYSQL_PASSWORD=admin -e MYSQL_DATABSE=jbdb mysql:latest\n</code></pre> <p>Or use the <code>docker-compose.yml</code> under the mysql folder.</p> <pre><code>docker-compose -f docker-compose.yml up &amp;\n</code></pre> <p>Product documentation. The default configuration for MySQL can be found in <code>/etc/mysql/my.cnf</code>.</p>"},{"location":"postgres/","title":"PostgreSQL","text":"<p>Postgresql is open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.</p> <p>PostgreSQL features ACID (Atomicity, Consistent, Isolation and Durability) properties. It has indexes (primary/unique), updatable views, triggers, foreign keys (FKs) and even stored procedures (SPs)</p>"},{"location":"postgres/#value-propositions","title":"Value propositions","text":"<ul> <li>Oldest open source RDBMS and advanced one, and is object-relational database</li> <li>Large dataset</li> <li>A lot of data types and support unstructured data too.</li> <li>Adopted in microservice world.</li> <li>Run in multiple process so better for horizontal scaling</li> <li>Suited for applications with high volume of both reads and writes</li> <li>Consider PostgreSQL for any application that might grow to enterprise scope, with complex queries and frequent write operations. </li> <li>Includes <code>pgbench</code> to be used to create data and also to stress the database. </li> </ul>"},{"location":"postgres/#architecture-for-ha","title":"Architecture for HA","text":"<p>For HA the principle is to try to get a second server to take over quickly in case of primary server failure, or to have several nodes to serve the same data. Multiple solutions exist to support HA, the synchronous one consider that a data-modifying transaction is not considered committed until all servers have committed the transaction.</p> <p>The PostgreSQL stack is comprised of a primary and replica services. WAL records are exchanged between primary and replicas.</p> <p></p> <p>Streaming replication is pushing changes from a primary PostgreSQL instance to its replicas.</p> <ul> <li>shared disk (hardware) failover: avoids synchronization overhead by having only one copy of the database. standby server is able to mount and start the database as though it were recovering from a database crash</li> <li>File system - block storage writes to the standby must be done in the same order as those on the primary.</li> <li>Write-Ahead Log Shipping</li> <li>Logical replication PostgreSQL logical replication constructs a stream of logical data modifications from the Write-Ahead Log. Logical replication allows replication of data changes on a per-table basis</li> <li>Trigger-Based Primary-Standby Replication asynch replication from primary to standby servers</li> <li>SQL-Based Replication Middleware:  a program intercepts every SQL query and sends it to one or all servers</li> <li>Synchronous Multimaster Replication each server accept write requests,  modified data is transmitted from the original server to every other server before each transaction commits.</li> <li> <p>Asynchronous Multimaster Replication each server works independently, and periodically communicates with the other servers to identify conflicting transactions which can be resolved by DB admin.</p> </li> <li> <p>Product documentation on warm standby</p> </li> </ul>"},{"location":"postgres/#my-projects-using-postgresql","title":"My Projects using postgresql","text":"<ul> <li>Vaccine order mgr</li> <li>In this project there is a copy of Quarkus - panache - postgresql quickstart with settings to access remote postgresql on IBM Cloud and kubernetes template for a secret to get URL, user and password to access the DB.  </li> <li>Autonomous Car Ride</li> <li>CDC Debezium demo with Flink</li> </ul>"},{"location":"postgres/#setup","title":"Setup","text":""},{"location":"postgres/#run-postgresql-locally-with-docker","title":"Run PostgreSQL locally with docker","text":"<ul> <li> <p>Verify docker last image tag</p> </li> <li> <p>Using docker command:</p> </li> </ul> <pre><code>docker run --ulimit memlock=-1:-1 -it --rm=true --memory-swappiness=0 --name pgdb -e POSTGRES_USER=pguser -e POSTGRES_PASSWORD=passw0rd -e POSTGRES_DB=bettertodo -p 5432:5432 postgres:latest\n</code></pre> <p>Or set the environment variables <code>POSTGRESQL_USER,POSTGRESQL_HOST, POSTGRESQL_PWD</code> in the .env script and then use the command: <code>source .env</code>.</p> <p>Under the deployment/docker/postgresql folder there are a set of commands to run postgresql with docker</p> <ul> <li>start the docker image for the database: <code>./startPostgresqlLocal.sh</code></li> <li> <p>Start bash in a postgres image to access psql: <code>./startPsql.sh LOCAL</code></p> </li> <li> <p>Another way is to use docker compose:</p> </li> </ul> <pre><code> postgresql:\n    container_name: postgres\n    hostname: postgres\n    image: postgres\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: pgpwd\n      POSTGRES_DB: ordersdb\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./database:/var/lib/postgresql/data/\n</code></pre> <p>If you need to add table creation script to run when the container starts, add the following lines in the volumes</p> <pre><code>      # copy the sql script to create tables\n      - ./sql/create_tables.sql:/docker-entrypoint-initdb.d/create_tables.sql\n</code></pre> <p>The official PostgreSQL Docker image allows us to place SQL files in the <code>/docker-entrypoint-initb.d</code> folder, and the first time the service starts, it will import and execute those SQL files.</p>"},{"location":"postgres/#kubernetes-deployment","title":"Kubernetes deployment","text":"<p>There is a kubernetes operator for postgresql CloudNativePG that can be install with install instructions:</p> <pre><code>kubectl create ns cnpg-system\nkubectl apply --server-side -f \\\n  https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.3.yaml\n# verify, as it takes sometime the first time\nkubectl describe deployment -n cnpg-system cnpg-controller-manager\n</code></pre> <p>See the Cloudnative-pg Quickstart in the git repo.  The default configuration of the CloudNativePG operator comes with a Deployment of a single replica, which is suitable for most demonstrations.</p> <ul> <li> <p>Define a PG cluster, see pg-cluster. By default, the operator will install the latest available minor version of the latest major version of PostgreSQL when the operator was released.   <pre><code>kubectl apply -f pg-cluster.yaml\n</code></pre></p> </li> <li> <p>As we need to access data files or may be load scripts to psql shell, we need to create PV and PVC:   <pre><code>kubectl apply -f pgadmin-pv-pvc.yaml \n</code></pre></p> </li> <li> <p>To deploy the PGadmin web app use the deployment: <code>deployment/k8s/postgresql/pgadmin-deploy.yaml</code>.    <pre><code>kubectl apply -f pgadmin-deploy.yaml\n</code></pre></p> </li> </ul> <p>Cloud Native Postgresql K8S operator. to deploy Posgresql using CRD. and installation.</p> <p>Access to the psql cli via:</p> <pre><code>kubectl get pods -n pgdb\nkubectl exec -ti -n pgdb pg-cluster-1 -c postgres -- psql -x -c \"SHOW timezone\"  \nkubectl exec -ti -n pgdb pg-cluster-1 -c postgres  -- bash\n</code></pre>"},{"location":"postgres/#pgadmin","title":"pgAdmin","text":"<p>The docker compose includes the pgAdmin UI at address http://localhost:5050. See user and password in docker compose. Register a server by using postgres as hostname, user (app) and password.</p> <p>When using kubernetes do a port-forward (see user and password as defined in pgadmin-deploy.yaml):</p> <pre><code>osascript -e 'tell app \"Terminal\" to do script \"kubectl  port-forward svc/pgadmin-service 5050:80 -n pgdb \"'\n</code></pre> <ul> <li>We can create a new database (dvdrental for example) and load definition and data from a backup which is a tar file.</li> </ul> <p></p> <ul> <li>Use the <code>Query Tool</code> to execute SQL command.</li> </ul> <p>We can load csv file in a table. Example from cab_rides for flink study: </p> <ol> <li>Be sure the csv file is mounted inside the docker container</li> <li>Create a table matching the columns of the csv with the expected types</li> </ol> <pre><code>CREATE TABLE cab_rides(\ncab_id VARCHAR(20), \ncab_plate VARCHAR(20), \ncab_type VARCHAR(20), \ndriver_name VARCHAR(100), \nongoing_trip VARCHAR(4), \npickup_location VARCHAR(20), \ndestination VARCHAR(20), \npassenger_count INTEGER\n);\n</code></pre> <ol> <li> <p>Enter a query like</p> <pre><code>COPY cab_rides FROM '/tmp/data_files/cab_rides.csv' DELIMITER ',';\n# Or if the first raw has a header\nCOPY cab_rides FROM '/' DELIMITER ',' CSV HEADER;\n</code></pre> </li> </ol> pg_read_server_files role <p>While copy it may be possible to get the error: Only roles with privileges of the \"pg_read_server_files\" role may COPY from a file.</p> <ul> <li>Can also use the backup and restore functions on the database to load data and schema: use the upload file button on top left of the user interface.</li> </ul>"},{"location":"postgres/#some-psql-commands","title":"Some psql commands","text":"<p>See psqk commands documentation.</p> <pre><code># connect to the container\ndocker exec -ti pgdb bash\n# OR\nkubectl exec -ti pg-cluster-1 -- sh\n# start psql using the user specified\npsql -U pguser -d dbname\n# for k8s deployment: first exec to the pod: k exec -ti -n pgdb pg-cluster-1 -c postgres  -- bash\npsql \"host=pg-cluster-rw user=app dbname=app password=apppwd\"\n# switch to another DB\n\\c dbname\n# list existing DBs\n\\l\n# list available tables\n\\dt \n# describe table\n\\d table_name\n# list schemas\n\\dn\n# list available funtions\n\\df\n# views\n\\dv\n# list users\n\\du\n# command history\n\\s\n# execute previoud commmand\n\\g\n# Execute a SQL query\nselect * from public.tablename;\ncreate table customers(id integer not null primary key, name varchar(20));\n</code></pre> <p>See puzzles</p> <p>Then start <code>psql</code> using the following command: </p> <pre><code>PGPASSWORD=$POSTGRESQL_PASSWORD psql -h postgresql -d $POSTGRESQL_DATABASE -U $POSTGRESQL_USER\npsql (10.12)\nType \"help\" for help.\n\nvaccinedb=&gt;\n</code></pre> <p>To access the database with pgadmin running locally, to the remote DB, we need to do port forwarding as:</p> <pre><code>oc get pods\noc port-forward postgres-5f449ccd95-tclb6 15432:5432\n</code></pre> <p>Then in the Quarkus app or in env file define properties like:</p> <pre><code>export QUARKUS_DATASOURCE_USERNAME=postgres\nexport QUARKUS_DATASOURCE_PASSWORD=postgres1234\nexport POSTGRESQL_DBNAME=postgres\nexport QUARKUS_DATASOURCE_JDBC_URL=jdbc:postgresql://localhost:15432/postgres\n</code></pre>"},{"location":"postgres/#hibernate-orm","title":"Hibernate ORM","text":"<p>See code for order management</p>"},{"location":"postgres/#use-python-to-interact-with-the-db","title":"Use python to interact with the DB","text":"<ul> <li>install psycopg2</li> </ul> <pre><code>pip install psycopg2\n</code></pre> <ul> <li>Reading table content</li> </ul> <pre><code>\n</code></pre> <ul> <li>Writing new content</li> </ul> <pre><code>\n</code></pre>"},{"location":"postgres/#more-readings","title":"More readings","text":"<ul> <li>How to Run PostgreSQL and pgAdmin Using Docker</li> </ul>"},{"location":"cassandra/readme/","title":"Cassandra Summary","text":"<p>In this article we do a quick summary of Cassandra capabilities in the context of data persistence and how to deploy it on Openshift.</p> <p>Cassandra addresses linear scalability and high availability to persist a huge data set. It uses replication to multiple nodes managed in cluster, even deployable  across datacenters.</p>"},{"location":"cassandra/readme/#concepts","title":"Concepts","text":"<p>Cassandra uses a ring based Distributed Hash Table servers but without finder or routing tables. Keys are stored as in DHT to the next server with key &gt; key id, replicated on the 2 next servers too.  There is one ring per data center.  The coordinator forward the query to a subset of replicas for a particular key. Every server that could be a coordinator needs to know the 'key to server' assignment. Two data placement strategies:</p> <ul> <li>simple strategy: use two kinds of partitioners:<ul> <li>random, which does chord like hashing</li> <li>byte ordered, which assigns range of keys to servers: easier for range queries</li> </ul> </li> <li>network topology strategy for multiple data centers: it supports different configuration, like 2 replicas of each key per decision center.</li> </ul> <p>First  replica is placed according to the Partitioner logic, and make sure to store the other replica to different rack, to avoid a rack failure will make all copies of key not available.  Go around the ring clockwise until encountering a server in a different rack.</p> <p>Snitches: is a mechanism to map ip addresses to racks in DC. Cassandra support such configuration.</p> <p>Client sends write to one coordinator node in Cassandra cluster. Coordinator may per key or per client or per query. It uses partitioner to send query to all replica nodes responsible for key. The process to write should be fast and not involving lock on resource. It should not involve read and disk seeks. Write operations are always successful, even in case of failure: the coordinator uses the Hinted Handoff mechanism (it assumes ownership of the key until being sure it is supported by the replica), as it writes to other replicas and keeps the write locally until the down replica comes back up. If all the replicas are done, the Coordinator buffers writes for few hours.</p> <p>Here are some key concepts of Cassandra to keep in mind for this implementation:</p> <ul> <li>Cluster:  the set of nodes potentially deployed cross data centers, organized as a 'ring'.</li> <li>Keyspace: like a schema in SQL DB. It is the higher abstraction object to contain data. The important keyspace attributes are the Replication Factor, the Replica Placement Strategy and the Column Families.</li> <li>Column Family: they are like tables in Relational Databases. Each Column Family contains a collection of rows which are represented by a Map&gt;. The key gives the ability to access related data together <li>Column \u2013 A column is a data structure which contains a column name, a value and a timestamp. The columns and the number of columns in each row may vary in contrast with a relational database where data are well structured.</li>"},{"location":"cassandra/readme/#cassandra-deployment","title":"Cassandra deployment","text":""},{"location":"cassandra/readme/#deploying-on-docker-edge-for-desktop-kubernetes","title":"Deploying on \"Docker Edge for desktop\" kubernetes","text":"<p>For development work we use the Docker Edge version of Docker and enable Kubernetes (see this note from docker web site).</p> <p>Then you can use our script <code>deployCassandra.sh</code> under the <code>scripts</code> folder BUT be sure to adapt the yaml settings in the folder <code>cassandra</code> to limit the replicas for what you need. The steps are the same as for kubernetes deployment as described in next section.</p>"},{"location":"cassandra/readme/#deployment-on-kubernetes","title":"Deployment on Kubernetes","text":"<p>Deploying stateful distributed applications like Cassandra is not easy. You will leverage the kubernetes cluster to support high availability and deploy c7a to the worker nodes.</p> <p></p> <p>We also recommend to be familiar with this kubernetes tutorial on how to deploy Cassandra with Stateful Sets.</p> <p>You can reuse the yaml config files under <code>cassandra</code> folder to configure a Service to expose Cassandra externally, create static persistence volumes, and use the StatefulSet to deploy Cassandra image.</p> <p>You can also use our script <code>deployCassandra.sh</code> under the <code>scripts</code> folder to automate this deployment.</p> <p>To summarize the steps are:</p> <ol> <li> <p>Connect to kubernetes or Openshift:  </p> <p>We are using one namespace called 'greencompute'.</p> </li> <li> <p>Create Cassandra headless service, so application can access via KubeDNS. If you do wish to connect an application to cassandra, use the KubeDNS value of <code>cassandra-svc.greencompute.svc.cluster.local</code>, or <code>cassandra-svc</code> or <code>cassandra-0</code>. The alternate solution is to use Ingress rule and set a hostname as cassandra.green.case. The <code>casssandara-ingress.yml</code> file defines such Ingress.</p> <pre><code>$ kubectl apply -f cassandra/cassandra-service.yaml --namespace greencompute\n$ kubectl get svc cassandra-svc -n greencompute\n\n  NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\ncassandra-svc   ClusterIP    None          &lt;none&gt;        9042/TCP   12h\n</code></pre> </li> <li> <p>Create static persistence volumes to keep data for cassandra: you need the same number of PV as there are cassandra nodes (here 3 nodes)</p> </li> </ol> <pre><code>$ kubectl apply -f cassandra/cassandra-volumes.yaml\n\n$ kubectl get pv -n greencompute | grep cassandra\n  cassandra-data-1  1Gi  RWO  Recycle   Bound       greencompute/cassandra-data-cassandra-0 12h\n  cassandra-data-2  1Gi  RWO  Recycle   Available                                           12h\n  cassandra-data-3  1Gi  RWO  Recycle   Available   \n</code></pre> <ol> <li>Create the StatefulSet, which defines a cassandra ring of 3 nodes. The cassandra image used is coming from dockerhub public repository.</li> </ol> <p>If you are using your own namespace name or you change the service name, modify the service name and namespace used in the yaml :</p> <pre><code>env:\n  - name: CASSANDRA_SEEDS\n    value: cassandra-0.cassandra-svc.greencompute.svc.cluster.local\n</code></pre> <p>Cassandra seed is used for two purposes:</p> <ul> <li>Node discovery: when a new cassandra node is added (which means when deployed on k8s, a new pod instance added by increasing the replica number), it needs to find the cluster, so here the seed is set the svc name</li> <li>Assist on gossip convergence: by having all of the nodes in the cluster gossip regularly with the same set of seeds. It ensures changes are propagated regularly.</li> </ul> <p>Here it needs to reference the headless service we defined for Cassandra deployment.</p> <pre><code>  $ kubectl apply -f cassandra/cassandra-statefulset.yaml  -n greencompute\n  $ kubectl get statefulset -n greencompute\n\n  NAME                                        DESIRED   CURRENT   AGE\n  cassandra                                   3         3         12h\n</code></pre> <ul> <li>Connect to the pod to assess the configuration is as expected. </li> </ul> <pre><code>  $ kubectl get pods -o wide -n greencompute\n  NAME          READY     STATUS    RESTARTS   AGE       IP              NODE\n  cassandra-0   0/1       Running   0          2m        192.168.35.93   169.61.151.164\n\n  $ kubectl exec -tin greencompute cassandra-0 -- nodetool status\n\n  Datacenter: DC1\n  ===============\n  Status=Up/Down\n  |/ State=Normal/Leaving/Joining/Moving\n  --  Address          Load       Tokens        Owns (effective)  Host ID                               Rack\n  UN  192.168.212.174  257.29 KiB  256          100.0%            ea8acc49-1336-4941-b122-a4ef711ca0e6  Rack1\n</code></pre> <p>The string \"UN\", means for Up and Normal state.</p>"},{"location":"cassandra/readme/#removing-cassandra-cluster","title":"Removing cassandra cluster","text":"<p>We are providing a script for that <code>./scripts/deleteCassandra.sh</code> which remove the stateful, the pv, pvc and service</p> <pre><code>grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\\n    &amp;&amp; kubectl delete statefulset -l app=cassandra -n greencompute \\\n    &amp;&amp; echo \"Sleeping $grace\" \\\n    &amp;&amp; sleep $grace \\\n    &amp;&amp; kubectl delete pvc,pv,svc -l app=cassandra\n</code></pre>"},{"location":"cassandra/readme/#performance-considerations","title":"Performance considerations","text":"<p>The resource requirements for higher performance are:</p> <ul> <li>A minimum of 32GB RAM (JVM + Linux memory buffers to cache SSTable) + <code>memory.available</code> defined in Kubernetes Eviction policy + Resources needed by k8s components that run on every worker node (e.g. proxy)</li> <li>A minimum of 8 processor cores per Cassandra Node with 2 CPUs per core (16 vCPU in a VM)</li> <li>4-8 GB JVM heap, recommend trying to keep heaps limited to 4 GB to minimize garbage collection pauses caused by large heaps.</li> <li>Cassandra needs local storage to get best performance. Avoid to use distributed storage, and prefer hostPath or localstorage. With distributed storage like a Glusterfs cluster you may have 9 replicas (3x Cassandra replica factor which is usually 3). </li> </ul> <p>Cassandra nodes tend to be IO bound rather than CPU bound:</p> <ul> <li>Upper limit of data per node &lt;= 1.5 TB for spinning disk and &lt;= 4 TB for SSD</li> <li>Increase the number of nodes to keep the data per node at or below the recommended capacity</li> <li>Actual data per node determined by data throughput, for high throughput, you need to limit the data per node.</li> </ul> <p>The use of Vnodes is generally  considered  to be a good practice as they eliminate the need to perform manual token assignment, distribute workload across all nodes in a cluster when nodes are added or removed. It helps rebuilding dead nodes faster. Vnode reduces the size of SSTables which can improve read performance. Cassandra best practices set the number of tokens per Cassandra node to 256.</p> <p>Avoid getting multiple node instances on the same physical host, so use <code>podAntiAffinity</code> in the StatefulSet spec.</p> <pre><code>spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            topologyKey: \"kubernetes.io/hostname\"\n</code></pre>"},{"location":"cassandra/readme/#high-availability","title":"High availability","text":"<p>Within a cluster the number of replicas in the statefulset is at least 3 but can be increased to 5 when code maintenance is needed. The choice for persistence storage is important, and the backup and restore strategy of the storage area network used.</p> <p>When creating connection to persist data into a keyspace, you specify the persistence strategy and number of replicas at the client code level. Mostly using properties file.</p> <pre><code>p.setProperty(CASSANDRA_STRATEGY, \"SimpleStrategy\");\np.setProperty(CASSANDRA_REPLICAS, \"3\");\n</code></pre> <p>When deploying on Staging or test cluster, ring topology, SimpleStrategy is enough: additional replicas are placed on the next nodes in the ring moving clockwise without considering topology, such as rack or datacenter location: a cluster can spread over availability zones and datacenters. For HA and production deployment NetworkTopologyStrategy is needed: replicas are placed in the same datacenter but on distinct racks.</p> <p>For the number of replicas, it is recommended to use 3 per datacenter.</p> <p>The <code>spec.env</code> parameters in the statefulset defines the datacenter name and rack name too.</p>"},{"location":"cassandra/readme/#code","title":"Code","text":"<p>We have done two implementations for persisting <code>asset</code> data into Cassandra, one using Cassandra client API and one with SpringBoot cassandra repository API.</p>"},{"location":"cassandra/readme/#cassandra-client-api","title":"Cassandra client API","text":"<p>The code is under <code>refarch-reefer-ml</code> project and can be loaded into Eclipse. This component is deployed as container inside a kubernetes cluster like Openshift. See code explanation, how to build and run in this note</p> <p>In the pom.xml we added the following dependencies to get access to the core driver API:</p> <p><pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n  &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;\n  &lt;version&gt;3.1.4&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> The DAO code is <code>CassandraRepo.java</code> and it basically connects to the Cassandra cluster when the DAO class is created... <pre><code>Builder b = Cluster.builder().addContactPoints(endpoints);\ncluster = b.build();\nsession = cluster.connect();\n</code></pre> The trick is in the endpoints name. We externalize this setting in a configuration properties and use the cassandra-svc name when deploy in ICP.</p> <p>It is possible also to create keyspace and tables by API if they do not exist by building CQL query string and use the session.execute(aquery) method. See this section below</p>"},{"location":"cassandra/readme/#define-assets-table-structure-with-cql","title":"Define Assets Table Structure with CQL","text":"<p>Using the csql tool we can create space and table. To use <code>cqlsh</code> connect to cassandra pod: <pre><code>$ kubectl exec -tin greencompute cassandra-0 cqlsh\n</code></pre> You are now in cqlsh shell and you can define assets table under keyspace <code>assetmonitoring</code>:</p> <p><pre><code>sqlsh&gt;  create keyspace assetmonitoring with replication={'class':'SimpleStrategy', 'replication_factor':1};\nsqlsh&gt; use assetmonitoring;\nsqlsh:assetmonitoring&gt; create TABLE assets(id text PRIMARY KEY, os text, type text, ipaddress text, version text, antivirus text, current double, rotation int, pressure int, temperature int, latitude double, longitude double);\n</code></pre> Add an index on the asset operating system field and one on type. <pre><code>CREATE INDEX ON assetmonitoring.assets (os);\nCREATE INDEX ON assetmonitoring.assets (type);\n</code></pre></p> <p>If you reconnect to the pod using cqlsh you can assess the table using <pre><code>describe tables\n\ndescribe assets\n</code></pre></p>"},{"location":"cassandra/readme/#some-useful-cql-commands","title":"Some useful CQL commands","text":"<pre><code># See the table schema\ncqlsh&gt; describe table assets;\n\n# modify a table structure adding a column\ncqlsh&gt; alter table assets add flowRate bigint;\n\n# change column type. example the name column:\ncqlsh&gt; alter table assets alter name type text;\n\n# list content of a table  \ncqlsh&gt; select id,ipaddress,latitude,longitude from assets;\n\n# delete a table\ncqlsh&gt; drop table if exists assets;\n</code></pre>"},{"location":"cassandra/readme/#use-cassandra-java-api-to-create-objects","title":"Use Cassandra Java API to create objects","text":"<ul> <li>Create keyspace:  <pre><code>StringBuilder sb =\n    new StringBuilder(\"CREATE KEYSPACE IF NOT EXISTS \")\n      .append(keyspaceName).append(\" WITH replication = {\")\n      .append(\"'class':'\").append(replicationStrategy)\n      .append(\"','replication_factor':\").append(replicationFactor)\n      .append(\"};\");\n\n    String query = sb.toString();\n    session.execute(query);\n</code></pre></li> <li> <p>Create table  <pre><code>StringBuilder sb = new StringBuilder(\"CREATE TABLE IF NOT EXISTS \")\n         .append(TABLE_NAME).append(\"(\")\n         .append(\"id uuid PRIMARY KEY, \")\n         .append(\"temperature text,\")\n         .append(\"latitude text,\")\n         .append(\"longitude text);\");\n\n       String query = sb.toString();\n       session.execute(query);\n</code></pre></p> </li> <li> <p>insert data: there is no update so if you want a strict insert you need to add \"IF NOT EXISTS\" condition in the query.</p> </li> </ul>"},{"location":"cassandra/readme/#issues","title":"Issues","text":"<ol> <li>When having the cassandra replica set to more than two, the cassandra operations are not happenning in parallel on both the pods at the same time. Some operations are happenning on one node and some others on the other node. Due to this, inconsistent data is retrieved since both the tables do not have the same data.</li> </ol>"},{"location":"cassandra/readme/#future-readings","title":"Future Readings","text":"<ul> <li>10 steps to set up a multi-data center Cassandra cluster on a Kubernetes platform</li> <li>IBM Article: Scalable multi-node Cassandra deployment on Kubernetes Cluster</li> <li>Running Cassandra on Kubernetes</li> </ul>"},{"location":"debezium/","title":"CDC with Debezium studies","text":""},{"location":"debezium/#tutorial-summary","title":"Tutorial summary","text":"<p>The Debezium tutorial is here and summarized in this note.</p>"},{"location":"debezium/#things-to-remember","title":"Things to remember:","text":"<ul> <li>Debezium is a Kafka connect connector, one dedicated per database: Postgresql, MySQL. </li> </ul>"},{"location":"debezium/#tutorial-steps","title":"Tutorial steps","text":"<p>The commands are provided in the Makefile under code/debezium-tutorial.</p> <ol> <li>The labs setup:     <pre><code>make start_zookeeper\nmake start_kafka\nmake start_mysql\nmake start_mysql_client\n</code></pre></li> </ol> Potential issues to address <p>When trying to connect to mysql server with the client, the root user is denied access. One way is to ensure the 'root' user (or the specific user you're using for Debezium) has privileges granted for the correct host.     <pre><code># remote connect via\ndocker exec -ti mysql bash\n# start local mysql, using debezium as password\nmysql -u root -p\n# Modify privilege with the SQL:\nCREATE USER 'root'@'%' IDENTIFIED BY 'debezium'\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;\nFLUSH PRIVILEGES;\n</code></pre></p> <ol> <li> <p>Registrer debezium connector to start monitoring the database server's binlog.     <pre><code>curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors/ -d@config.json\n</code></pre></p> <p>When you register a connector, it generates a large amount of log output in the Kafka Connect container.</p> </li> <li> <p>The connector performs a snapshot, looking at the table schema, reading all the rows, then it may create kafka topics, and then it transitions to continuously reading the binlog.</p> </li> <li>The topics created match the tables in the database:</li> </ol> topic name role dbserver1 The schema change topic to which all of the DDL statements are written. dbserver1.inventory.products Captures change events for the products table in the inventory database. dbserver1.inventory.products_on_hand Captures change events for the products_on_hand table in the inventory database. dbserver1.inventory.customers Captures change events for the customers table in the inventory database. dbserver1.inventory.orders Captures change events for the orders table in the inventory database. <ol> <li> <p>View events created     <pre><code>docker run -it --rm --name watcher --link zookeeper:zookeeper --link kafka:kafka quay.io/debezium/kafka:3.2 watch-topic -a -k dbserver1.inventory.customers\n# or\nmake view_customers\n</code></pre></p> <p>We should see two records, one for the key and one for the payload.</p> <p>The event has two parts: a schema and a payload. The schema contains a Kafka Connect schema describing what is in the payload</p> </li> <li> <p>The envelop provides metadata about what the event represents:</p> <ul> <li>op: A required field that contains a string value describing the type of operation</li> <li>before: An optional field that, if present, contains the state of the row before the event occurred.</li> <li>after: An optional field that, if present, contains the state of the row after the event occurred. </li> <li>source: A required field that contains a structure describing the source metadata for the event, which in the case of MySQL, contains several fields: the connector name, the name of the binlog file where the event was recorded, the position in that binlog file where the event appeared, the row within the event (if there is more than one), the names of the affected database and table, the MySQL thread ID that made the change, whether this event was part of a snapshot, and, if available, the MySQL server ID, and the timestamp in seconds.</li> <li>ts_ms: An optional field that, if present, contains the time (using the system clock in the JVM running the Kafka Connect task) at which the connector processed the event.</li> </ul> </li> <li> <p>The JSON converter includes the key and value schemas in every message, so it does produce very verbose events. While with Avro, it will persist the schema in a remote schema registry, and send the binary unique identifier of the schema.</p> </li> <li> <p>Update table records in the mySQL client     <pre><code>UPDATE customers SET first_name='Anne Marie' WHERE id=1004;\n</code></pre></p> </li> <li> <p>Deleting a record:     <pre><code>delete from addresses where customer_id=1004;\ndelete from customers WHERE id=1004;\n</code></pre></p> <p>The key records is the same, while the payload has an after that is null. <pre><code>  \"payload\": {\n    \"before\": {\n        \"id\": 1004,\n        \"first_name\": \"Anne Marie\",\n        \"last_name\": \"Kretchmar\",\n        \"email\": \"annek@noanswer.org\"\n    },\n    \"after\": null,\n    \"source\": \n</code></pre></p> </li> <li> <p>Stop the kafka connector,      <pre><code>docker stop connect\n</code></pre></p> <p>add new records <pre><code>INSERT INTO customers VALUES (default, \"Sarah\", \"Thompson\", \"kitt@acme.com\");\nINSERT INTO customers VALUES (default, \"Kenneth\", \"Anderson\", \"kander@acme.com\");\n</code></pre></p> </li> <li> <p>Clean up     <pre><code>\n</code></pre></p> </li> </ol>"},{"location":"debezium/#to-keep-in-mind","title":"to keep in mind","text":"<p>If Kafka is set up to be log compacted, it will remove older messages from the topic if there is at least one message later in the topic with same key. This last event is called a tombstone event, because it has a key and an empty value. This means that Kafka will remove all prior messages with the same key. Even though the prior messages will be removed, the tombstone event means that consumers can still read the topic from the beginning and not miss any events.</p> <p>The Kafka Connect service automatically manages tasks for its registered connectors. Therefore, if it goes offline, when it restarts, it will start any non-running tasks. This means that even if Debezium is not running, it can still report changes in a database.</p>"},{"location":"sql/","title":"SQL basics","text":"<p>In the SQL language we can differentiate the Data Definition Language (DDL), which are statements to create, change, or deleting tables, from Data Modification Language (DML), which are used to define statements to modify the data and to do not change the metadata.</p> <p>The figure below illustrates some common basic patterns.</p> <p></p> <p>For deeper tutorial see the sqltutorial.org site. The Employee DDL is in DDL folder.</p>"},{"location":"sql/#starting-sql-editor","title":"Starting SQL editor","text":"<p>As a main playground, I use Postgresql. See setup section to run it with docker or kubernetes. The URL should be http://localhost:5050 user admin@example.com</p> <ul> <li>Register a server by using the name of the rw service as hostname (pg-cluster-rw) user app, and see the secret for the password.</li> <li>Use the <code>Query Tool</code> to execute SQL command.</li> </ul>"},{"location":"sql/#simple-getting-started","title":"Simple getting started","text":""},{"location":"sql/#exercises-from-medium-articles","title":"Exercises from medium articles","text":"<ul> <li> <p>The SQL Questions with Detailed Answers (Step-by-Step), see the sql scripts in the code/postgresql/medium1 folder.</p> </li> <li> <p>Start docker compose as it mount the <code>./medium1</code> folder into <code>/tmp/scripts</code>. </p> </li> <li>Create the tables in the postgres db</li> </ul> <pre><code># under /tmp/scripts\npsql -U postgres -f create_db.sql\n</code></pre> <ul> <li>insert records <code>psql -U postgres -f inser-record-1.sql</code></li> <li>Write an SQL query to report all customers who never order anything. Use left join to take all the values from the left table and the common rows from the right table. The left join was performed on the Customer table because we want all the Customers with their Orders.</li> </ul> <pre><code>select * from customers left join orders on customers.id = orders.customer_id;\n\nid | name  | id | customer_id \n----+-------+----+-------------\n  3 | Sam   |  1 |           3\n  1 | Joe   |  2 |           1\n  2 | Henry |    |            \n  4 | Max   |    |   \n\nselect name from customers left join orders on customers.id = orders.customer_id where orders.customer_id is null; \n</code></pre>"},{"location":"sql/#create-customers","title":"Create customers","text":"<p>Here is the complete SQL you can run in psql</p> <pre><code>CREATE TABLE customers (customer_id varchar(8) PRIMARY KEY, lastname varchar(40) NOT NULL, firstname varchar(40) NOT NULL, zipcode varchar(5), country varchar(40), status integer);\nINSERT INTO customers (customer_id,lastname,firstname,zipcode,country,status) VALUES\n('C01','Builder','Bob','95050','USA',1),\n('C02','Destroyer','Bill','95050','USA',1),\n('C03','Climber','Jack','95052','USA',1),\n('C04','Messenger','John','95052','USA',1);\n</code></pre> <p>or use the command</p> <pre><code>psql postgres://$POSTGRES_USER:$POSTGRES_PWD@$POSTGRES_HOST/$POSTGRES_DB -a -f /home/dll/customer.sql\n</code></pre>"},{"location":"sql/#create-products","title":"Create products","text":"<p>Products define fresh product with controlled temperature and humidity to control for the travel.</p> <pre><code>CREATE TABLE products (\n    product_id varchar(64) NOT NULL PRIMARY KEY,\n    description varchar(100),\n    target_temperature REAL,\n    target_humidity_level REAL,\n    content_type integrer\n);\n\nINSERT INTO products(product_id,description,target_temperature,target_humidity_level,content_type) VALUES\n('P01','Carrots',4,0.4,1),\n('P02','Banana',6,0.6,2),\n('P03','Salad',4,0.4,1),\n('P04','Avocado',6,0.4,1),\n('P05','Tomato',4,0.4,2);\n</code></pre> <pre><code>psql postgres://$POSTGRES_USER:$POSTGRES_PWD@$POSTGRES_HOST/$POSTGRES_DB -a -f /home/dll/product.sql\n</code></pre>"},{"location":"sql/#table-creation","title":"Table creation","text":"<p>For primary key try to using numerical type and Postgresl sequence like:</p> <pre><code>CREATE TABLE players (player_id SERIAL PRIMARY_KEY, age smallint not null)\n</code></pre> <p>Use CHECK to put constraint on column and between column</p> <pre><code>hire_date DATE check (hire_date &gt; birthday)\n</code></pre>"},{"location":"sql/#alter-table","title":"Alter table","text":"<pre><code>-- drop a constraint\nalter table people alter column name drop not null\n</code></pre> <p>Alter table in postgresql - doc</p>"},{"location":"sql/#basic-command-on-a-customers-table","title":"Basic command on a customers table","text":"<pre><code>select * from customers;\nselect distinct(name) from customers;\nselect count(distinct(rate)) from films;\n\n-- how many customer has the name bob and are older than 18\nselect count(*) from customer where name = 'bob' and age &gt;= 18;\n-- sort by salary\nselect name, salary from customers order by salary DESC;\n\n-- limit the number of records returned\nSELECT name, salary FROM customers ORDER BY salary DESC LIMIT 10;\n\n-- BETWEEN\nselect count(*) from payment where amount between 8 and 9;\nselect * from payment where payment_date between '2007-02-01' and '2007-2-15';\n\n-- IN to test value in a list of options\nselect count(*) from payment where amount in(0.99, 1.98, 1.99);\n\n-- LIKE and ILIKE (case non-sensitive) to do pattern matching on string\nselect * from customer where first_name ilike 'J%';\n\n-- Modify the table\nalter table customers add column email varchar(100);\n-- update a unique record\nupdate customers  set email='max@email.com' where id = 4;\n\n-- Aggregate min, max, avg, count,...\nselect round(avg(replacement_code),3) from film;\n\n-- GROUP BY combined with aggregate. Who is the customer spending the most\nselect customer_id, sum(amount) from  payment group by customer_id order by sum(amount) DESC;\n\n-- get the day with the most transactions\nselect DATE(payment_date), sum(amount) from  payment group by  DATE(payment_date) order by sum(amount) DESC;\n\n-- HAVING to allow us to use the aggregate result to filter the result along with group by\nselect customer_id, sum(amount) from  payment  \nwhere staff_id = 2 \ngroup by  customer_id having sum(amount) &gt;= 110;\n</code></pre> <p>See also the postgres study for information to run SQL on a local postgresql started with docker compose or kubernetes.</p>"},{"location":"sql/#exercises-on-the-dvdrental-database","title":"Exercises on the dvdrental database","text":"<p>See postgres to restore the database schema and data from the tar file.</p> <ul> <li> <p>How many payment transactions were greater than $5.00?</p> <pre><code>select count(*) from payment where amount &gt; 5\n</code></pre> </li> <li> <p>How many actors have a first name that starts with the letter P?</p> <p><pre><code>select count(first_name) from actor where first_name like 'P%';\n</code></pre> * How many unique districts are our customers from?</p> <pre><code>select count(distinct(district)) from address;\n-- Retrieve the list of names for those distinct districts \nselect distinct(district) from address where district is not null and district != '' order by district asc;\n</code></pre> </li> <li> <p>How many films have a rating of R and a replacement cost between $5 and $15?</p> <pre><code>select count(*) from film where  rating = 'R' and replacement_cost between 5 and 15;\n</code></pre> </li> <li> <p>How many films have the word Truman somewhere in the title?</p> <pre><code>select count(*) from film where title ilike '%truman%';\n</code></pre> </li> <li> <p>Which staff member processes the biggest number of transactions?</p> <pre><code>select staff_id, count(amount) from  payment group by  staff_id;\n</code></pre> </li> <li> <p>What is the avg replacement cost per film rating?</p> <pre><code>select rating, round(avg(replacement_cost),2) from  film group by  rating;\n</code></pre> </li> <li> <p>Customer eligible for platinum status having more than 40 transactions</p> <pre><code>select customer_id, count(amount) from  payment group by  customer_id having count(amount) &gt;= 40 ORDER by count(amount);\n</code></pre> </li> <li> <p>Customer who spent more than 100$ with a given staff</p> <pre><code>select customer_id, sum(amount) from  payment  where staff_id = 2 group by  customer_id having sum(amount) &gt;= 100;\n</code></pre> </li> </ul>"},{"location":"sql/sql/","title":"Intermediate SQL","text":"<p>This chapter is dedicated to summarize common major SQL constructs. The following tutorial has a lot of very good examples for deeper study.</p> <ul> <li>Protect against null for value and substitute using COALESCE</li> </ul> <pre><code>select product, (price - coalesce(discount,0)) from products;\n</code></pre>"},{"location":"sql/sql/#play-with-timestamp","title":"Play with timestamp","text":"<pre><code>-- get the year of a date\nselect extract(year from payment_date) as myyear from payment;\n-- can use month, quarter, month, day\n-- get how old is a record\nselect AGE(payment_date) from payment;\n--\nselect TO_CHAR(payment_date, \"MM-DD-YYYY\") from payment;\n</code></pre>"},{"location":"sql/sql/#mathematical-functions","title":"Mathematical functions","text":"<p>Compute things from columns. Examples from postgresql</p> <ul> <li>Get list of students who scored better than average grade</li> </ul> <pre><code>-- avg grade\nselect AVG(grade) from test_scores\n-- solution with subquery\nselect student, grade from test_scrores\nwhere grade &gt; ( select AVG(grade) from test_scores);\n</code></pre>"},{"location":"sql/sql/#self-join","title":"Self-join","text":"<p>A query in which table is joined to itself: used to compare values in a column of rows within the same table. Need to use aliases. Get the employe's name and the name of his manager:</p> <pre><code>select emp.name, report.name as manager from employees as emp\njoin employees as report \non emp.emp_id = report.report_id \n</code></pre> <ul> <li>find the films with the same length presented as pair</li> </ul> <pre><code>from f1.title, f2.title, f1.length from film as f1\ninner join film as f2\non f1.film_id != f2.film_id and f1.length = f2.length\n</code></pre> <ul> <li>Compare the various amounts of films per movie rating</li> </ul> <pre><code>select\nsum(CASE rating\n    WHEN 'R' THEN 1\n    ELSE 0\nEND) as R,\nsum(CASE rating\n    WHEN 'PG' THEN 1\n    ELSE 0\nEND) as PG,\nsum(CASE rating\n    WHEN 'PG-13' THEN 1\n    ELSE 0\nEND) as PG13\nfrom film\n</code></pre>"},{"location":"sql/sql/#add-views","title":"Add views","text":"<p>It is a stored query to be executed often.</p> <pre><code>create view customer_info as \nSELECT ... -- the sql to repeat\n</code></pre>"},{"location":"sql/sql/#remove-duplicates","title":"Remove duplicates","text":"<pre><code>-- search for same transaction\nSELECT transaction_id, COUNT(transaction_id)\nFROM Transaction__Table\nGROUP BY transaction_id\nHAVING COUNT(transaction_id) &gt; 1;\n-- \n</code></pre>"},{"location":"sql/sql/#with","title":"WITH","text":"<p>The WITH clause allows to create temporary named result sets that exist only for the duration of the query. It is like creating a temporary view or defining a sub-query that we can reference multiple times. It is named CTE for Common Table Expressions.</p> <pre><code>WITH cte_name AS (\n    SELECT column1, column2\n    FROM table_name\n    WHERE condition\n)\nSELECT * FROM cte_name;\n</code></pre> <p>Can be used for recursive queries: Like get the list of employees of their manager up to the CEO:</p> <pre><code>WITH RECURSIVE employee_hierarchy AS (\n    -- Base case: get top-level employees\n    SELECT employee_id, `name`, manager_id, 1 as level\n    FROM employees\n    WHERE manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive case: get subordinates\n    SELECT e.employee_id, e.name, e.manager_id, h.level + 1\n    FROM employees e\n    JOIN emp_hierarchy h ON e.manager_id = h.employee_id\n)\nSELECT * FROM emp_hierarchy;\n</code></pre>"},{"location":"sql/sql/#joins","title":"Joins","text":"<p>A JOIN combines rows from two or more tables based on a related column between them. There are several types of JOINs:</p>"},{"location":"sql/sql/#inner-join","title":"INNER JOIN","text":"<p>Te goal is to build a projection of all elements in table A also present in table B. Get all columns of both tables:</p> <pre><code>SELECT * FROM table_a INNER JOIN table_b on table_a.column = table_b.column_b\n</code></pre> <pre><code>-- Returns only the matching rows from both tables\nSELECT employees.name, departments.dept_name\nFROM employees\nINNER JOIN departments \n    ON employees.dept_id = departments.id;\n</code></pre> <p>It is used to find matches between tables, or to get the complete records only.</p> <ul> <li>Get the top 10 customer name who do the most renting</li> </ul> <pre><code>select first_name, last_name, count(*) from payment \ninner join customer \non payment.customer_id = customer.customer_id \ngroup by first_name, last_name \norder by count(*) desc limit 10;\n</code></pre> <pre><code>-- get email address of customer leaving in california\nselect district, email from customer\njoin address\non address.address_id = customer.address_id\nwhere address.district = 'California'\n</code></pre>"},{"location":"sql/sql/#left-outer-join","title":"LEFT (OUTER) JOIN","text":"<p>OUTER JOIN or LEFT JOIN is used to deal with column only in the left side table.  Returns all rows from left table, matching row from right table, or null if no match: </p> <pre><code>SELECT employees.name, departments.dept_name\nFROM employees\nLEFT JOIN departments \n    ON employees.dept_id = departments.id;\n-- Will show all employees, even those without departments\n</code></pre> <p>It is used for checking for missing records, or to report with all records from a main table.</p> <p>Another example:</p> <pre><code>select title, inventory_id, store_id from film \nleft join inventory \non film.film_id = inventory.film_id\n</code></pre> <p>Some films may not be in the current inventory.</p>"},{"location":"sql/sql/#right-outer-join","title":"RIGHT (OUTER) JOIN:","text":"<p>Returns all rows from right table and matching rows from left table.</p> <pre><code>SELECT employees.name, departments.dept_name\nFROM employees\nRIGHT JOIN departments \n    ON employees.dept_id = departments.id;\n-- Will show all departments, even those without employees\n</code></pre>"},{"location":"sql/sql/#full-outer-join","title":"FULL OUTER JOIN","text":"<p>Returns all rows from both tables.</p> <pre><code>-- full outer join - customer never buy anything\nselect * from customer \nfull outer join payment \non payment.customer_id = customer.customer_id\nwhere customer.customer_id IS null;\n-- set of records that are in the left table, present or not in the right\n</code></pre> <p>Another example:</p> <pre><code>-- film in catalog not in the inventory\nselect title, inventory_id, store_id from film \nleft join inventory \non film.film_id = inventory.film_id\nwhere inventory_ID IS null\n</code></pre> <p></p>"},{"location":"sql/sql/#combining-joins","title":"Combining Joins","text":"<pre><code>-- Example with three tables\nSELECT \n    e.name AS employee_name,\n    d.dept_name,\n    p.project_name\nFROM employees e\nINNER JOIN departments d \n    ON e.dept_id = d.id\nLEFT JOIN projects p \n    ON e.project_id = p.id\nWHERE d.location = 'New York';\n</code></pre>"},{"location":"sql/sql/#some-practices","title":"Some practices","text":"<ul> <li>Always use table aliases for better readability</li> <li>Specify the JOIN type explicitly (don't rely on implicit joins)</li> <li>Be careful with OUTER JOINs as they can impact performance</li> <li>Use appropriate indexes on join columns</li> <li> <p>Consider the order of joins when working with multiple tables</p> </li> <li> <p>What are the film with a given actor?</p> <pre><code>select  title,first_name, last_name from film_actor as fa\ninner join actor\non actor.actor_id = fa.actor_id\ninner join film\non fa.film_id = film.film_id\nwhere last_name = 'Wahlberg' and first_name = 'Nick'\n</code></pre> </li> <li> <p>Film returned on a specific date: uses subquery, and joins</p> <pre><code>select film_id,title from film\nwhere film_id in\n(select inventory.inventory_id from rental \ninner join inventory\non inventory.inventory_id = rental.inventory_id\nwhere rental.return_date between '2005-05-29' and ' 2005-05-30')\nORDER BY title\n</code></pre> </li> </ul>"},{"location":"sql/sql/#over","title":"OVER","text":"<p>The OVER clause in SQL is a powerful operator that transforms how aggregate functions (like SUM, AVG, COUNT, MAX, MIN) and other window functions (like ROW_NUMBER, RANK, LEAD, LAG) operate. Instead of grouping rows into a single summary row (like a traditional GROUP BY clause), the OVER clause allows these functions to perform calculations across a set of related rows while still returning each individual row of the original result set.</p> <p>For example calculating the running total of the transactions seen so far, by also generating each transaction:</p> <pre><code>tx_id, tx_date, amount, running_total\n74  \"2024-06-14\"    626.69  626.69\n2   \"2024-06-20\"    793.97  1420.66\n90  \"2024-06-26\"    397.17  1817.83\n88  \"2024-06-30\"    290.69  2108.52\n78  \"2024-07-01\"    245.87  2354.39\n</code></pre> <pre><code>SELECT transaction_id, transaction_date, amount, sum(amount) over (order by transaction_date) as running_total \nFROM sales;\n</code></pre> <ul> <li> <p>Common Use Cases for the OVER Clause:</p> <ul> <li>Running Totals: Calculating a cumulative sum or count as you go through the data.</li> <li>Moving Averages: Calculating the average of a specific number of preceding and/or following rows.</li> <li>Ranking: Assigning ranks to rows within a group (e.g., ROW_NUMBER(), RANK(), DENSE_RANK(), NTILE()).</li> <li>Percentage of Total: Calculating what percentage each row's value contributes to a total for its group.</li> <li>Comparing to Previous/Next Rows: Using LAG() to get a value from a previous row or LEAD() to get a value from a subsequent row within a partition.</li> <li>First/Last Value in a Group: Retrieving the FIRST_VALUE() or LAST_VALUE() in a defined window.</li> </ul> </li> </ul> <p>Examples of queries</p> Problem Queries Calculate the total sales for each salesperson SELECT SalesPerson, SaleDate, SaleAmount, SUM(SaleAmount) OVER (PARTITION BY SalesPerson) AS TotalSalesPerPerson FROM Sales; Rank sales within each salesperson's record SELECT SalesPerson, SaleDate, SaleAmount, ROW_NUMER() OVER (PARTITION BY SalesPerson ORDER BY SaleAmount DESC) AS SaleRank  FROM Sales; Calculate a running total of sales for each salesperson, ordered by date SELECT SalesPerson, SaleDate, SaleAmount, SUM(SaleAmount) OVER (PARTITION BY SalesPerson ORDER BY SaleDat) AS RunningTotal FROM Sales;"},{"location":"sql/sql/#see-classical-sql-puzzles","title":"See classical SQL puzzles","text":"<p>See puzzles with instructions and code explanations in each sql file.</p>"}]}